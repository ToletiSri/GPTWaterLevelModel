{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Carnival Demo - GPTWaterLevelModel","metadata":{}},{"cell_type":"code","source":"!rm -rf GPTWaterLevelModel\n!git clone https://github.com/ToletiSri/GPTWaterLevelModel.git\n","metadata":{"execution":{"iopub.status.busy":"2024-07-07T04:13:38.004327Z","iopub.execute_input":"2024-07-07T04:13:38.005161Z","iopub.status.idle":"2024-07-07T04:13:41.026080Z","shell.execute_reply.started":"2024-07-07T04:13:38.005127Z","shell.execute_reply":"2024-07-07T04:13:41.025004Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Cloning into 'GPTWaterLevelModel'...\nremote: Enumerating objects: 33, done.\u001b[K\nremote: Counting objects: 100% (33/33), done.\u001b[K\nremote: Compressing objects: 100% (27/27), done.\u001b[K\nremote: Total 33 (delta 11), reused 5 (delta 2), pack-reused 0\u001b[K\nUnpacking objects: 100% (33/33), 90.07 KiB | 572.00 KiB/s, done.\n","output_type":"stream"}]},{"cell_type":"code","source":"cd GPTWaterLevelModel","metadata":{"execution":{"iopub.status.busy":"2024-07-07T04:13:41.028004Z","iopub.execute_input":"2024-07-07T04:13:41.028306Z","iopub.status.idle":"2024-07-07T04:13:41.034888Z","shell.execute_reply.started":"2024-07-07T04:13:41.028272Z","shell.execute_reply":"2024-07-07T04:13:41.034017Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/working/GPTWaterLevelModel\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Create and train the model","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport torch\nimport config as cfg\nfrom gpt import GPTWaterLevelModel","metadata":{"execution":{"iopub.status.busy":"2024-07-07T04:13:41.035857Z","iopub.execute_input":"2024-07-07T04:13:41.036089Z","iopub.status.idle":"2024-07-07T04:13:44.837822Z","shell.execute_reply.started":"2024-07-07T04:13:41.036069Z","shell.execute_reply":"2024-07-07T04:13:44.836977Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Read the values from excel\ndf = pd.read_csv('teraterm_sensor.csv', sep=',', header=None)\nsensor_data = torch.tensor(df.values)\nprint(sensor_data.shape, sensor_data.dtype)\nprint(sensor_data[1:10, :])","metadata":{"execution":{"iopub.status.busy":"2024-07-07T04:13:44.839024Z","iopub.execute_input":"2024-07-07T04:13:44.839610Z","iopub.status.idle":"2024-07-07T04:13:44.935652Z","shell.execute_reply.started":"2024-07-07T04:13:44.839577Z","shell.execute_reply":"2024-07-07T04:13:44.934780Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"torch.Size([326, 3]) torch.float64\ntensor([[48.8853,  2.2496,  7.2400],\n        [48.8853,  2.2496,  7.2300],\n        [48.8853,  2.2496,  7.2200],\n        [48.8853,  2.2496,  7.2100],\n        [48.8853,  2.2496,  7.2000],\n        [48.8853,  2.2496,  7.1900],\n        [48.8853,  2.2496,  7.1800],\n        [48.8853,  2.2496,  7.1700],\n        [48.8853,  2.2496,  7.1600]], dtype=torch.float64)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Train and test splits\nn = int(1.0*len(sensor_data)) # Take 100%, currently we dont have sufficient data for train and test\ntrain_data = sensor_data[:n]\nval_data = sensor_data[n:]\nprint(train_data.shape, train_data.dtype)\nprint(val_data.shape, val_data.dtype)","metadata":{"execution":{"iopub.status.busy":"2024-07-07T04:13:44.937790Z","iopub.execute_input":"2024-07-07T04:13:44.938076Z","iopub.status.idle":"2024-07-07T04:13:44.943711Z","shell.execute_reply.started":"2024-07-07T04:13:44.938051Z","shell.execute_reply":"2024-07-07T04:13:44.942594Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"torch.Size([326, 3]) torch.float64\ntorch.Size([0, 3]) torch.float64\n","output_type":"stream"}]},{"cell_type":"code","source":"# Read parameters from configuration\ndevice = cfg.device\nblock_size = cfg.block_size\nbatch_size = cfg.batch_size\neval_iters = cfg.eval_iters\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2024-07-07T04:13:44.944813Z","iopub.execute_input":"2024-07-07T04:13:44.945111Z","iopub.status.idle":"2024-07-07T04:13:44.954654Z","shell.execute_reply.started":"2024-07-07T04:13:44.945085Z","shell.execute_reply":"2024-07-07T04:13:44.953726Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"# data loading\ndef get_batch(split, batch_size = 1):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    x, y = x.float().to(device), y.float().to(device)\n    return x, y\n\n@torch.no_grad()\ndef estimate_loss(model):\n    out = {}\n    model.eval()\n    for split in ['train']: #, 'val'\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(split, batch_size)\n            Y = Y[:, :, 2] # Pass only water level as expected output\n            logits, loss = model(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()\n    return out","metadata":{"execution":{"iopub.status.busy":"2024-07-07T04:13:44.955607Z","iopub.execute_input":"2024-07-07T04:13:44.955870Z","iopub.status.idle":"2024-07-07T04:13:44.966478Z","shell.execute_reply.started":"2024-07-07T04:13:44.955846Z","shell.execute_reply":"2024-07-07T04:13:44.965543Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Model Training\nfrom gpt import GPTWaterLevelModel\nmodel = GPTWaterLevelModel(800).to(device)\n# print the number of parameters in the model\nprint(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')\n\n# create a PyTorch optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=cfg.learning_rate)\n\nfor iter in range(cfg.max_iters):\n\n    #every once in a while evaluate the loss on train and val sets\n    if iter % cfg.eval_interval == 0 or iter == cfg.max_iters - 1:\n        losses = estimate_loss(model)\n        print(f\"step {iter}: loss {losses['train']:.4f}\")    # , val loss {losses['val']:.4f}     \n\n    # sample a batch of data\n    xb, yb = get_batch('train', batch_size)\n    yb = yb[:, :, 2] # Pass only water level as expected output\n\n    # evaluate the loss\n    logits, loss = model(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n    \ntorch.save(model.state_dict(), 'saved_model.pth')","metadata":{"execution":{"iopub.status.busy":"2024-07-07T04:13:44.967479Z","iopub.execute_input":"2024-07-07T04:13:44.967748Z","iopub.status.idle":"2024-07-07T04:14:46.053080Z","shell.execute_reply.started":"2024-07-07T04:13:44.967725Z","shell.execute_reply":"2024-07-07T04:14:46.052116Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"5.121056 M parameters\nstep 0: loss 6.7323\nstep 100: loss 5.8139\nstep 200: loss 5.7792\nstep 300: loss 4.0265\nstep 400: loss 0.2870\nstep 500: loss 0.0537\nstep 600: loss 0.0261\nstep 700: loss 0.0168\nstep 800: loss 0.0117\nstep 900: loss 0.0088\nstep 999: loss 0.0070\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Generate the next few water levels based on current water levels","metadata":{}},{"cell_type":"code","source":"# sample a batch of data\nxb, _ = get_batch('train',1)\n# Take the first 8 samples from the batch and generate next 8 values\nCurrentWaterLevels = xb[:,:8,:] # Take the first 8 samples. Data = latitude, longitude, water_level of first 8 elements\nnext_coordinates = xb[:,8:12,0:2] # Take the co-ordinates of next 8 samples. Data = latitude, longitude of next 8 elements\n# Predict the next 8 water levels at the given next 8 co-ordinates\npredicted_water_levels = model.generate(CurrentWaterLevels, 4, next_coordinates) \nprint(f'Current 8 water levels = {[round(value, 4) for value in CurrentWaterLevels[:,:,-1].tolist()[0]]}' )\nprint(f'Actual next 4 water levels = {[round(value, 4) for value in xb[:,8:12,-1].tolist()[0]]}' )\nprint(f'Predicted next 4 water levels = {[round(value,4) for value in predicted_water_levels[:,8:,-1].tolist()[0]]}' )","metadata":{"execution":{"iopub.status.busy":"2024-07-07T04:14:46.054581Z","iopub.execute_input":"2024-07-07T04:14:46.055128Z","iopub.status.idle":"2024-07-07T04:14:46.255164Z","shell.execute_reply.started":"2024-07-07T04:14:46.055099Z","shell.execute_reply":"2024-07-07T04:14:46.254199Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Current 8 water levels = [6.14, 6.13, 6.12, 6.11, 6.1, 6.09, 6.08, 6.07]\nActual next 4 water levels = [6.06, 6.05, 6.04, 6.03]\nPredicted next 4 water levels = [6.06, 6.05, 6.04, 6.03]\n","output_type":"stream"}]}]}